{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0e5ddc-3795-4adc-9094-401094c5adb9",
   "metadata": {},
   "source": [
    "# 手动实现softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cdbcb8-3218-45e4-9536-29ca0b3d04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0e6c2b-1c10-49b7-bcb1-e1f1926db6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac45bd6-5d21-46a4-93bb-b09e84230cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 7., 9.]]),\n",
       " tensor([[ 6.],\n",
       "         [15.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb5b62-ce0e-410e-aa03-777e6fb00400",
   "metadata": {},
   "source": [
    "### 定义softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6158ed37-0f85-4675-bef6-21b6d023518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94442cbd-2c66-4547-aa82-b608887c0802",
   "metadata": {},
   "source": [
    "这里来试一下这个函数\n",
    "生成一个形状为 (2 行，5 列) 的张量，形状如输出所示，元素服从均值为 0、标准差为 1的正态分布（随机数，可正可负）。\n",
    "使用softmax函数后查看结果，所有元素非负，总和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14b2645-b94f-4234-ad45-421a75b5619b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1424, 0.1812, 0.5674, 0.0610, 0.0479],\n",
       "         [0.1703, 0.1624, 0.0974, 0.0353, 0.5345]]),\n",
       " tensor([1.0000, 1.0000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd45be-4cce-4c70-bc75-840c44cd4118",
   "metadata": {},
   "source": [
    "### 神经网络前向传播过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199933c-a6aa-4b2f-8a02-efc33ad43814",
   "metadata": {},
   "source": [
    "reshape((-1, W.shape[0])) 中，-1 是 “自动计算维度” 的占位符：无论原始 X 是多少维（比如可能是图片的三维张量 [batch, height, width]），都会被重塑为 (样本数, 输入特征数) 的二维张量。\n",
    "\n",
    "在经过reshape后X的形状为\"batch_size * 784\",因为输入为28*28=784个像素，W的形状为\"input_samples * num_output\",因此为 784 * 10, b的形状为10\n",
    "\n",
    "计算后进行softmax的操作，得到每一个feature的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bff6801-a43b-4512-9f94-50c7f9eb5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc87d31-6e22-4bac-b927-398243e07791",
   "metadata": {},
   "source": [
    "### 定义损失函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221db2e8-3d63-4c93-b3ba-41ca418992b9",
   "metadata": {},
   "source": [
    "首先y代表真实值，其中有两个样本，样本1输入类别0，样本2属于类别2\n",
    "y_hat是预测值，形状为2*3，代表两个样本分别为3个类别的可能性\n",
    "最后一行是 PyTorch 的高级索引操作，作用是：从y_hat中取出 “每个样本真实类别对应的预测概率”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b16e2f6-ffba-403e-8d97-82b5ec7c22fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885158f-5f1a-4e14-986b-2b9b0890745d",
   "metadata": {},
   "source": [
    "接下来实现交叉熵\n",
    "\n",
    "$ loss = - log(y_{true})$\n",
    "\n",
    "\n",
    "range(len(y_hat))：生成样本索引（比如y_hat有 2 个样本，就生成[0, 1]），作用和前面的[0, 1]一致，但更通用（样本数变化时无需手动改）。\n",
    "\n",
    "y_hat[range(len(y_hat)), y]：再次提取 “每个样本真实类别对应的预测概率”（和步骤 2 逻辑相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675f8f5d-11be-4537-a7e7-9f17813623dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
